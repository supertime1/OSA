{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "sleep_staging.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supertime1/OSA/blob/main/sleep_staging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qenih5OC1WGZ"
      },
      "source": [
        "#1.Import Dependency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MF9Qp1QZL7co"
      },
      "source": [
        "import os\n",
        "import mne\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import pickle"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ivd9p141UMC"
      },
      "source": [
        "#2.Process the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VI6ZHOcPL7c2"
      },
      "source": [
        "def load_data(file_path, sf=128, epoch_duration=30):\n",
        "\n",
        "    ecg_samples = []\n",
        "    ecg_labels = []\n",
        "    total_epoches = 0\n",
        "    \n",
        "    for signal_file in glob.glob(file_path + '*[0-9].edf'):\n",
        "        \n",
        "        ecg_epoches = []\n",
        "        \n",
        "        data = mne.io.read_raw_edf(signal_file)\n",
        "        ecg_ch = [i for i, v in enumerate(data.info.ch_names) if v == 'ECG']\n",
        "        ecg_signal = data.get_data()[ecg_ch[0]]\n",
        "        \n",
        "        num_of_sample_per_epoch = sf * epoch_duration\n",
        "        num_of_epoches = len(ecg_signal) // (num_of_sample_per_epoch)\n",
        "        total_epoches += num_of_epoches\n",
        "        \n",
        "        print(f'{signal_file[-12:]} has {num_of_epoches} epoches')\n",
        "\n",
        "        for i in range(num_of_epoches):\n",
        "            ecg_epoch = ecg_signal[i*num_of_sample_per_epoch : (i+1)*num_of_sample_per_epoch]\n",
        "            ecg_epoches.append(ecg_epoch)\n",
        "        ecg_samples.append(ecg_epoches)\n",
        "    \n",
        "    for label_file in glob.glob(file_path + '*stage.txt'):\n",
        "        print(f'reading {label_file}')\n",
        "        ecg_labels.append(np.loadtxt(label_file))\n",
        "        \n",
        "    assert len(ecg_samples) == len(ecg_labels)\n",
        "\n",
        "    for i in range(len(ecg_samples)):\n",
        "        new_length = len(ecg_labels[i])\n",
        "        ecg_samples[i] = ecg_samples[i][:new_length]\n",
        "    \n",
        "    return ecg_samples, ecg_labels, total_epoches\n",
        "\n",
        "fp = \"C:/Users/57lzhang.US04WW4008/Downloads/ucd/files/\"\n",
        "ecg_samples, ecg_labels, total_epoches = load_data(fp, 128, 30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcCZ0XlUPpOQ"
      },
      "source": [
        "with open(\"C:/Users/57lzhang.US04WW4008/Downloads/ucd/files/processed_data/ecg_samples.pkl\", \"wb\") as fp:\r\n",
        "    pickle.dump(ecg_samples, fp)\r\n",
        "\r\n",
        "with open(\"C:/Users/57lzhang.US04WW4008/Downloads/ucd/files/processed_data/ecg_labels.pkl\", \"wb\") as fp:\r\n",
        "    pickle.dump(ecg_labels, fp)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uMXE-6lL7c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eec54be3-f4f6-4fe2-9f02-211200d0b64f"
      },
      "source": [
        "# split by patient id \r\n",
        "def split_data(ecg_samples, ecg_labels, train_ratio, test_ratio):\r\n",
        "    test_num = round(len(ecg_samples) * test_ratio)\r\n",
        "    train_num = round((len(ecg_samples) - test_num) * train_ratio)\r\n",
        "    val_num = len(ecg_samples) - test_num - train_num\r\n",
        "\r\n",
        "    np.random.seed(seed=7)\r\n",
        "    np.random.shuffle(ecg_samples)\r\n",
        "    train_samples = ecg_samples[:train_num]\r\n",
        "    val_samples = ecg_samples[train_num:train_num+val_num]\r\n",
        "    test_samples = ecg_samples[-test_num:]\r\n",
        "\r\n",
        "    np.random.seed(seed=7)\r\n",
        "    np.random.shuffle(ecg_labels)\r\n",
        "    train_labels = ecg_labels[:train_num]\r\n",
        "    val_labels = ecg_labels[train_num:train_num+val_num]\r\n",
        "    test_labels = ecg_labels[-test_num:]\r\n",
        "\r\n",
        "    return train_samples, train_labels, val_samples, val_labels, test_samples, test_labels \r\n",
        "\r\n",
        "train_samples, train_labels, val_samples, val_labels, test_samples, test_labels = split_data(ecg_samples, ecg_labels, 0.8, 0.12)\r\n",
        "print(f'There are {len(train_samples)} subjects in training dataset')\r\n",
        "print(f'There are {len(val_samples)} subjects in validation dataset')\r\n",
        "print(f'There are {len(test_samples)} subjects in testing dataset')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 18 subjects in training dataset\n",
            "There are 4 subjects in validation dataset\n",
            "There are 3 subjects in testing dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liakZ4jYL7c7"
      },
      "source": [
        "def preprocess_data(num_epoch, epoch_duration, sf, ecg_samples, ecg_labels, oversample=True):\n",
        "    \"\"\"\n",
        "    preprocess data with the matched dimension for training\n",
        "    [num_epoch, 30*sampling_frequency, 1]\n",
        "\n",
        "    params:\n",
        "    epoch - number of epoches for each training sample\n",
        "    sf - sampling frequency of ECG signal\n",
        "    file_path - file path of raw EDF file\n",
        "    \"\"\"\n",
        "    model_signal_input = []\n",
        "    model_label_input = []\n",
        "\n",
        "    num_of_sample_per_epoch = sf * epoch_duration\n",
        "\n",
        "    for i in range(len(ecg_samples)):\n",
        "        \n",
        "        if oversample:\n",
        "            overlap = int(0.9 * num_epoch)\n",
        "            for j in range(len(ecg_samples[i])):\n",
        "                signal_segment = np.asarray(ecg_samples[i][j*(num_epoch - overlap): j*(num_epoch - overlap) + num_epoch])\n",
        "                if len(signal_segment) == num_epoch:\n",
        "                    new_signal_seg = np.reshape(signal_segment, (num_epoch, num_of_sample_per_epoch, 1))\n",
        "                    model_signal_input.append(new_signal_seg)\n",
        "                \n",
        "                # apply to labels as well\n",
        "                label_segment = np.asarray(ecg_labels[i][j*(num_epoch - overlap): j*(num_epoch - overlap) + num_epoch])\n",
        "                if len(label_segment) == num_epoch:\n",
        "                    model_label_input.append(label_segment)\n",
        "        \n",
        "        else:\n",
        "            for j in range(len(ecg_samples[i])):\n",
        "                signal_segment = np.asarray(ecg_samples[i][j*num_epoch: (j+1)*num_epoch]) \n",
        "                if len(signal_segment) == num_epoch:\n",
        "                    new_signal_seg = np.reshape(signal_segment, (num_epoch, num_of_sample_per_epoch, 1))\n",
        "                    model_signal_input.append(new_signal_seg)\n",
        "\n",
        "                # apply to labels as well\n",
        "                label_segment = np.asarray(ecg_labels[i][j*num_epoch: (j+1)*num_epoch])\n",
        "                if len(label_segment) == num_epoch:\n",
        "                    model_label_input.append(label_segment)\n",
        "        \n",
        "    print(f'shape of processed signal data: {np.asarray(model_signal_input).shape}')\n",
        "    print(f'shape of processed label data: {np.asarray(model_label_input).shape}')\n",
        "\n",
        "    return np.asarray(model_signal_input), np.asarray(model_label_input)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DyRhHbEyoT4"
      },
      "source": [
        "def helper(samples, labels):\r\n",
        "    for i in range(len(samples)):\r\n",
        "        print(f'{len(samples[i])}, {len(labels[i])}')  "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GD1pa8iYyLlO",
        "outputId": "d08b5f36-5ca2-4c9a-be6a-64f4fe998aa6"
      },
      "source": [
        "helper(ecg_samples, ecg_labels)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "882, 882\n",
            "768, 768\n",
            "774, 774\n",
            "789, 789\n",
            "826, 826\n",
            "711, 711\n",
            "864, 864\n",
            "752, 752\n",
            "916, 916\n",
            "748, 748\n",
            "893, 893\n",
            "925, 925\n",
            "908, 908\n",
            "913, 913\n",
            "721, 721\n",
            "811, 811\n",
            "787, 787\n",
            "900, 900\n",
            "822, 822\n",
            "907, 907\n",
            "861, 861\n",
            "808, 808\n",
            "838, 838\n",
            "813, 813\n",
            "852, 852\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_sRNpmNbsjB",
        "outputId": "19c94c51-49f2-4aac-b23d-99ddf387d27d"
      },
      "source": [
        "train_signal_input, train_label_input = preprocess_data(100, 30, 128, train_samples, train_labels, oversample=True)\r\n",
        "val_signal_input, val_label_input = preprocess_data(100, 30, 128, val_samples, val_labels, oversample=False)\r\n",
        "test_signal_input, test_label_input = preprocess_data(100, 30, 128, test_samples, test_labels, oversample=False)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of processed signal data: (1319, 100, 3840, 1)\n",
            "shape of processed label data: (1319, 100)\n",
            "shape of processed signal data: (33, 100, 3840, 1)\n",
            "shape of processed label data: (33, 100)\n",
            "shape of processed signal data: (24, 100, 3840, 1)\n",
            "shape of processed label data: (24, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBhTjlajO0JY"
      },
      "source": [
        "def join_labels(label_input):\r\n",
        "    for i in range(len(label_input)):\r\n",
        "        label_input[label_input > 1] = 2\r\n",
        "    return label_input"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfBTo1DySxHh"
      },
      "source": [
        "train_label_input_join = join_labels(train_label_input)\r\n",
        "val_label_input_join = join_labels(val_label_input)\r\n",
        "test_label_input_join = join_labels(test_label_input)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcFDcYX61O72"
      },
      "source": [
        "#3.Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c57gR-C4xJSq"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras import Model\r\n",
        "from tensorflow.keras.models import load_model \r\n",
        "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\r\n",
        "from tensorflow.keras.layers import Conv1D, BatchNormalization, Input, Add, Activation,\\\r\n",
        "MaxPooling1D,Dropout,Flatten,TimeDistributed,Bidirectional,Dense,LSTM, ZeroPadding1D, \\\r\n",
        "AveragePooling1D,GlobalMaxPooling1D, Concatenate, Permute, Dot, Multiply, RepeatVector,\\\r\n",
        "Lambda, Average\r\n",
        "from tensorflow.keras.initializers import glorot_uniform\r\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVnqfV5E2MfB"
      },
      "source": [
        "batch_size = 16\r\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_signal_input, train_label_input_join))\r\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_signal_input, val_label_input_join))\r\n",
        "train_dataset = train_dataset.cache()\r\n",
        "train_dataset = train_dataset.shuffle(1024).repeat().batch(batch_size, drop_remainder=True)\r\n",
        "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\r\n",
        "val_dataset = val_dataset.repeat().batch(batch_size, drop_remainder=True)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqvRMtVPTyE5"
      },
      "source": [
        "##Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wr7CnSvMTqvF"
      },
      "source": [
        "# callbacks\r\n",
        "log_dir = r\"C:\\Users\\57lzhang.US04WW4008\\Desktop\\OSA\\TensorBoard\\logs\\fit\\\\\" + \\\r\n",
        "          datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"test\"\r\n",
        "\r\n",
        "## confusion matrix callback\r\n",
        "def log_confusion_matrix(epoch, logs):\r\n",
        "    # Use the model to predict the values from the test_images.\r\n",
        "    \r\n",
        "    val_pred_raw = model.predict(val_signal_input)\r\n",
        "\r\n",
        "    val_pred = np.argmax(val_pred_raw, axis=-1)\r\n",
        "    #test_labels = val_labels.reshape([len(t_val_labels)])\r\n",
        "\r\n",
        "    # Calculate the confusion matrix using sklearn.metrics\r\n",
        "    cm = sklearn.metrics.confusion_matrix(val_label_input_join, val_pred)\r\n",
        "\r\n",
        "    figure = model_util.plot_confusion_matrix(cm, class_names=class_names, normalize=True)\r\n",
        "    cm_image = model_util.plot_to_image(figure)\r\n",
        "\r\n",
        "    # Log the confusion matrix as an image summary.\r\n",
        "    with file_writer_cm.as_default():\r\n",
        "        tf.summary.image(\"Confusion Matrix\", cm_image, step=epoch)\r\n",
        "\r\n",
        "class_names = ['Wake','NREM','REM']\r\n",
        "file_writer_cm = tf.summary.create_file_writer(log_dir + '/cm')\r\n",
        "cm_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)\r\n",
        "\r\n",
        "## tensorboard callback\r\n",
        "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\r\n",
        "\r\n",
        "## checkpoint callback\r\n",
        "filepath = r\"C:\\Users\\57lzhang.US04WW4008\\Desktop\\OSA\\models\\test-oversample-128Hz-{epoch:02d}-{loss:.4f}\"\r\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\r\n",
        "\r\n",
        "## early stop\r\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\r\n",
        "\r\n",
        "## learning rate decay callback\r\n",
        "lr_schedule = tf.keras.callbacks.LearningRateScheduler(model_util.decay)\r\n",
        "\r\n",
        "callbacks_list = [tensorboard_callback, cm_callback, checkpoint, early_stop, lr_schedule]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4t-6PrNT1I5"
      },
      "source": [
        "##Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yheFjSY82-c8",
        "outputId": "12fc6ddc-a890-4332-87da-1b2561018460"
      },
      "source": [
        "cnn = tf.keras.Sequential([\r\n",
        "    #1st Conv1D\r\n",
        "    tf.keras.layers.Conv1D(8, 1, strides=1, \r\n",
        "                          activation='relu'),\r\n",
        "    tf.keras.layers.BatchNormalization(),\r\n",
        "    tf.keras.layers.MaxPooling1D(pool_size=2,strides=2),\r\n",
        "    tf.keras.layers.Dropout(0.2),\r\n",
        "    #2nd Conv1D\r\n",
        "    tf.keras.layers.Conv1D(16, 3, strides=1,\r\n",
        "                          activation='relu'),\r\n",
        "    tf.keras.layers.BatchNormalization(),\r\n",
        "    tf.keras.layers.MaxPooling1D(pool_size=2,strides=2),\r\n",
        "    tf.keras.layers.Dropout(0.2),\r\n",
        "    #3rd Conv1D\r\n",
        "    tf.keras.layers.Conv1D(32, 3, strides=1,\r\n",
        "                          activation='relu'),\r\n",
        "    tf.keras.layers.BatchNormalization(),\r\n",
        "    tf.keras.layers.MaxPooling1D(pool_size=2,strides=2),\r\n",
        "    tf.keras.layers.Dropout(0.2),\r\n",
        "    #4th Conv1D\r\n",
        "    tf.keras.layers.Conv1D(64, 3, strides=1,\r\n",
        "                          activation='relu'),\r\n",
        "    tf.keras.layers.BatchNormalization(),\r\n",
        "    tf.keras.layers.MaxPooling1D(pool_size=2,strides=2),\r\n",
        "    tf.keras.layers.Dropout(0.2),\r\n",
        "    #5th Conv1D\r\n",
        "    tf.keras.layers.Conv1D(16, 1, strides=1,\r\n",
        "                          activation='relu'),\r\n",
        "    tf.keras.layers.BatchNormalization(),\r\n",
        "    #Full connection layer\r\n",
        "    tf.keras.layers.Flatten()\r\n",
        "])\r\n",
        "\r\n",
        "#combine with LSTM\r\n",
        "model = tf.keras.Sequential([\r\n",
        "        tf.keras.layers.TimeDistributed(cnn,input_shape=(100,3840,1)),                   \r\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256,return_sequences=True)),\r\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences = True)),\r\n",
        "        tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(8))\r\n",
        "])\r\n",
        "\r\n",
        "model.summary()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "time_distributed (TimeDistri (None, 100, 3808)         9776      \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 100, 512)          8325120   \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 100, 256)          656384    \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 100, 8)            2056      \n",
            "=================================================================\n",
            "Total params: 8,993,336\n",
            "Trainable params: 8,993,064\n",
            "Non-trainable params: 272\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsdPxG-S4u6W"
      },
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(),\r\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSaqMKILK74W",
        "outputId": "86d721c4-f424-49ec-e16a-e03affa4b51e"
      },
      "source": [
        "model.fit(train_dataset,\r\n",
        "          epochs=10,\r\n",
        "          steps_per_epoch=len(train_signal_input)//batch_size,\r\n",
        "          verbose=1,\r\n",
        "          validation_data=val_dataset,\r\n",
        "          validation_steps=len(val_signal_input)//batch_size\r\n",
        "          )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "12/82 [===>..........................] - ETA: 29s - loss: 1.5343 - accuracy: 0.4036"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}